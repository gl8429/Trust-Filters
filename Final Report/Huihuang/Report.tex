\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ificcvfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Add Named Entity Recognizer to Convolutional Neural Network }

\author{
Guangyu Lin\\
UTEID: gl8429\\
{\tt\small glin@utexas.edu} \\
\and
Ge Gao\\
UTEID: gg24984\\
{\tt\small gegao1118@utexas.edu} \\
\and
Huihuang Zheng\\
UTEID: hz4674\\
{\tt\small huihuang@utexas.edu} \\
}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%\tt\small secondauthor@i2.org}
%}

\maketitle
%\thispagestyle{empty}


%%%%%%%%% ABSTRACT
\begin{abstract}
In this work, our assumption is that adding Named Entity Recognizor (NER) \cite{nadeau2007survey} as a feature for Convoluional Neural Network can improve accuracy for NLP tasks involving named entities. To verify our assumption, we try to solve TREC question classification task \cite{li2002learning}. Our method adds Named Entity Recognizor (NER) as one dimension number to \textit{word2vec} expression, then the modified expression is used for training a CNN. Our experiment illustrated that our CNN + NER model can improve the accuracy of question type classification task compared to only CNN method. Although our method still can not get comparable performance of the state-of-the-art work, Our method is easier to train because the state-of-the-art work needs a large number of hand-code resources. 
\end{abstract}

%%%%%%%%% BODY TEXT

\section{Introduction}
Automatic question answering is an important information retrieval task in natural language processing and it is more challenging than purely searching task because it needs to ``understand'' what is asked before searching for answers. And to make machines ``understand'' a question is as challenging as it sounds like. One method that could help solve this problem is called Question Classification(QC). Question Classification(QC) is a task that given a question, some classifier will map this question to one of k limited classes, so that some semantic constraint will be put on this question, which provides machine with some basic information about the question which could hopefully help machine ``understand'' questions.

In 2000's TREC competition \cite{li2002learning}, participants were requested to develop a system to classify English questions based on some question categories. And after that, many remarkable results have been achieved. One of them is Li and Roth's work. Li and Roth develops a hierarchical classifier based on SNoW learning architecture(Carlson et al, 1999 \cite{carlson1999snow}; Roth, 1998 \cite{roth1998learning}) to solve the question classification task.

With the development of deep learning, machine learning models have been applied to NLP tasks and it turns out that many of these models significantly improved performance of original models without deep learning methods. One example is Yoon Kim's \cite{kim2014convolutional} work that applies convolutional neural networks (CNN) to sentence classification.

Since named entity is an important part of questions, especially when number of question types is large, but previous work did not make much use of the semantic information of a named entity. For example the named entity might be either a location or a person. This work makes use of Stanford's Named Entity Recognizer (NER) \cite{nadeau2007survey} and adds extra labels to vectors generated by \textit{word2vec}. Then we train convolutional neural network model with these new vectors. Our goal is to improve the performance of the CNN model for question classification, especially for questions related to named entity semantic meanings.

The main contribution of this work is that our method, which adds Name Entity Recognizer (NER) as one extra dimension for each \textit{word2vec}'s output vector, can improve top-1 accuracy in a 50-classes question classification task -- TREC fine types, which demonstrates that by combining NER and CNN, our method can improve CNN classification accuracy. Although our method still cannot outperform the state-of-the-art method which doesn't apply neural networks, our CNN classifier is easier to deploy because the non-neural approach method uses a large number of hand-coded resources. 

\section{Problem Definition and Algorithm}
\subsection{Task Definition}

\subsubsection{Named Entity Recognition}
Named Entity Recognition (NER) labels sequences of words in a text which are the names of things, such as person and company names, or gene and protein names. (See Tabel)

\subsubsection{Word2Vec}

\subsubsection{Convolutional neural network}

\subsection{Algorithm}




\section{Experiment}
\subsection{Methodology}

\subsubsection{Dataset}
Our method was evaluated by comparing accuracy in a question classification task named TREC \cite{li2006learning} with previous researchers' results. The task involves classifying a question into $k$ question types. They defined a two-layered taxonomy, which represents a  natural semantic classification for typical answers in the TREC task. The hierarchy contains 6 coarse classes (ABBREVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION and NUMERIC VALUE) and 50 fine classes. Each coarse  class contains a non-overlapping set of fine classes. We choose this dataset because of two reasons. The fact that Named Entity Recognizer can classify person, location may help us in this task. Second, in Kim's paper \cite{kim2014convolutional} and Li's paper \cite{li2006learning}, they also used this database and reported previous accuracy so it will be easy for us to compare. The TREC dataset has 5500 annotated questions for training and 500 questions as the test set.

\subsubsection{Setup}
We used \textit{word2vec} trained model from Mikolov \cite{mikolov2013efficient} on 100 billion words of Google News and from TODO: Guangyu please add the reference for word2vec. We built our framework using NER from Stanford NER software \cite{finkel2005incorporating} TODO: Guangyu, am I correct? 

and deep learning tool Theano \cite{Bastien-Theano-2012}. Since Kim's work\cite{kim2014convolutional} has Github published code using Theano (\url{https://github.com/yoonkim/CNN_sentence}), our method which combines CNN and NER was extended from their code. 

We compared our method with Kim's method of using CNN and previous best methods without using CNN in the TREC dataset. By comparing CNN and our CNN + NER method, we can evaluate the contribution of NER features in this task. In addition, because Kim's method didn't outperform previous methods without CNN, we need to compare previous methods.

\subsubsection{Evaluation}
In this paper, we use same metric as previous researchers. We count the number of correctly classified questions by two standards. Top-1 accuracy and top-5 accuracy. We define
$$ 
I_{i,j} = 
\left\{ 
  \begin{array}{ll}
    1 & \parbox{5cm}{Correct label of the $i$-th question is output within rank $j$ }\\
    \\
    0 & \parbox{3cm}{otherwise}
  \end{array} 
\right. 
$$
Here, classifiers will give $j$ labels for the $i$-th question. If the correct label for question $i$ is among those $j$ labels, $I_{i,j}$ will be set to 1. Top 1 accuracy is defined as $j = 1$, which is a usual precision definition. Top 5 accuracy is defined as $j = 5$, which allows multi-labels for classifiers. Formally, if we have n test samples, top-$k $ accuracy is defined as
$$
P_k = \frac{1}{n}\sum_{i=1}^{n} I_{i,k} 
$$

\subsection{Experimental Results}
We reimplemented Kim's CNN method as baseline. We train Kim's CNN and our CNN + NER in 2000 epochs. Then, we record the best accuracy in the 2000 epochs. Our result uses non-static CNN in Kim's paper because it got best result than other Kim's CNN model in this task. We compared with two methods without using CNN: 1, Hierarchical classifier by Li and Roth\cite{li2002learning} uses features unigram, POS, head chunks, NE, and semantic relations. 2, SVM$_s$ uses ni-bi-trigrams, wh word, head word, POS, parser, hypernyms, and 60 hand-coded rules as features from Silva et al \cite{silva2011symbolic}, which is the state-of-the-art in the best of our knowledge. Our test result for coarse classes was shown in table \ref{coarse}. Our test result for fine classes was shown in table \ref{fine}

\begin{table}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|}
  \hline  
  Method & Top-1 Accuracy & Top-5 Accuracy\\
  \hline
  Hierarchical classifier\cite{li2002learning} & 91.0 & 98.8 \\
  SVM$_S$ \cite{silva2011symbolic} & 95.0 & - \\
  %DCNN \cite{kalchbrenner2014convolutional} & 93.0 & - \\
  CNN \cite{kim2014convolutional} & 91.8 & 100.0 \\
  \hline
  CNN + NER (ours) & 91.8 & 100.0 \\
  \hline
\end{tabular}
}
\caption{Top-1 and top-5 accuracy for TREC coarse question types. }
\label{coarse}
\end{table}

\begin{table}

\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|}
  \hline  
  
  Method & Top-1 accuracy & Top-5 accuracy\\
  \hline
  Hierarchical classifier\cite{li2002learning} & 84.20 & 95.00 \\
  SVM$_S$ \cite{silva2011symbolic} & 90.8 & - \\
  CNN \cite{kim2014convolutional} & 80.2 & 90.60 \\
  \hline
  CNN + NER (ours) & 81.0 &  90.60 \\ 
  \hline
\end{tabular}
}
\caption{Top-1 and top-5 accuracy for TREC coarse question types.}
\label{fine}
\end{table}

\subsection{Discussion}
In table \ref{coarse}, we can see our CNN + NER model doesn't improve accuracy from Kim's only CNN model. However, when it is in a more complexed situation, the fine task in table \ref{fine}, we can see our model can slightly improved the top-1 accuracy (81.0 vs 80.2 in only CNN model). 

Comparing to non-neural approaches methods, which needs manually engineered features. CNN's approach is easier to train. Our method has higher top-1 and top-5 accuracy than the Hierarchical classifier method in coarse task. However in fine task, Li and Roth's method outperforms ours. According to the fact that we add NER's feature to CNN and improved the accuracy of fine task, we think that demonstrates the human defined features are helpful in fine types classification. 

Our CNN + NER model has not got comparable accuracy to the work of Silva et al. \cite{silva2011symbolic}. However, their classifier involves a large number of hand-code resources. For example, they have 60 hand-coded rules. CNN's training is simpler because we don't need those manually engineered features. 

\section{Related Work}
\subsection{Question classifiers}
In the ``Learning Question Classifier'' paper (Li and Roth, 2002 \cite{li2002learning}), in terms of the question classification task, the number of question types(k) could be either six or fifty depending on different classification criteria.  Li and Roth developed a machine learning method to classify questionsï¼Œ which is guided by a layered semantic hierarchy of answer types. They made use of a sequence of two simple classifiers to do the classification. The first classifies questions into coarse classes (six in total) and the second classifies questions into fine classes (fifty in total), which is dependent on the first one. Here is the structure of the hierarchical classifier by Li and Roth.
\includegraphics[width=0.5\textwidth]{structure.png}
\begin{center}
Figure1:  The hierarchical classifier 
\end{center}
Figure 1 shows the basic classification process by classifier developed by Li and Roth. A question will always be processed along a top-down path to be classified. And after the classification process, question type label(s) will be attached to the question.

In Li and Roth's classifier, each question is analyzed as a list of features so that they could be trained and tested for learning. They extracted some primitive features like words, pos tags and chunks (Abney, 1991 \cite{berwick1991principle}), named entity as well as some semantically related words. Based on these features, they compose and make some more complex features. Also, they make a semantically related word list for each of most question types. For example, ``far'' is in the semantically related words of ``distance'' so that if there is an occurrence of ``far'', and then the sensor for ``distance'' will be activated and the feature will be extracted.

A point that is worth pointing out is that there might be ambiguity for some specific questions. For example, a question like ``What do bats eat'' could either be classified to belong to food type or animal type, and both them make much sense. To solve this, Li and Roth allow multiple type labels to be attached to a single question.

\subsection{CNN on sentences classification}
Convolutional neural networks (CNN) model is a deep learning method which has achieved remarkable results in many fields. A CNN model was developed by Yoon Kim \cite{kim2014convolutional} for sentence classification which accepts word vectors as input and generates type labels as output.

Convolutional neural networks was originally invented for tasks in computer vision fields and was proved to be also effective in many NLP tasks. In Kim' model, they train one layer of filter on top of word vectors generated by word2vec developed by Google (Mikolov et al 2013 \cite{mikolov2013efficient}). In Kim's model, there are ``static'' and ``nonstatic'' models for model variation, where ``static'' means that the vectors are directly from word2vec and for ``nonstatic'', those vectors will also be tuned for each data set.



\section{baseline of Kim's model}
Because Kim's model that is available from his GitHub is for Google news, so we made some modifications based on his code and did the replicating experiment for TREC 10.

\section{Future Work}
This time we tried add NER features to CNN and improved CNN question classifier slightly, we think if we add more features, the CNN classifier can be more accurate.

\section{Conclusion}
In this work, we add Named Entity Recognizor (NER) as one dimension number to \textit{word2vec} expression. The modified expression can be used for training a CNN for NLP tasks involving named entity. Our experiment shows that our CNN + NER model can improve the accuracy of question type classification task compared to only CNN method. Although our method still can not get comparable performance of the state-of-the-art work, the state-of-the-art work needs a large number of hand-code resources. Our method is easier to train.
%-------------------------------------------------------------------------



{\small
\nocite{*}
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
