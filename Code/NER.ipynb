{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk,os\n",
    "import nltk.tag.stanford as st\n",
    "os.environ[\"CLASSPATH\"] = \"/Users/Lucifer/Documents/GraduateStudy/NLP/Trust-Filters/Code/stanford-ner-2014-06-16/stanford-ner.jar\"\n",
    "os.environ[\"STANFORD_MODELS\"] = \"/Users/Lucifer/Documents/GraduateStudy/NLP/Trust-Filters/Code/stanford-ner-2014-06-16/classifiers/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st1 = st.StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "st2 = st.StanfordNERTagger('english.conll.4class.distsim.crf.ser.gz')\n",
    "st3 = st.StanfordNERTagger('english.muc.7class.distsim.crf.ser.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LOC:state', 'O'), ('Which', 'O'), ('two', 'O'), ('states', 'O'), ('enclose', 'O'), ('Chesapeake', 'LOCATION'), ('Bay', 'LOCATION'), ('?', 'O')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'LOC:state Which two states enclose Chesapeake Bay ?'\n",
    "strings = st3.tag(sentence.split()) \n",
    "print(strings)\n",
    "# sentences = [retrieve(strings)]\n",
    "# sentences = [retrieve(strings)] + sentence.split()\n",
    "# print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LOC:state', 'O'), ('Which', 'O'), ('two', 'O'), ('states', 'O'), ('enclose', 'O'), ('Chesapeake', 'LOCATION'), ('Bay', 'LOCATION'), ('?', 'O')]\n",
      "Chesapeake_Bay\n"
     ]
    }
   ],
   "source": [
    "def retrieve(strings):\n",
    "    res = \"\"\n",
    "    for string in strings:\n",
    "        if string[1] != 'O':\n",
    "            res += string[0] + \"_\"\n",
    "    return res[:len(res) - 1]\n",
    "print(strings)\n",
    "print(retrieve(strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordIdxMap(fileName):\n",
    "    word_idx_map = dict()\n",
    "    index = 0\n",
    "    with open(fileName, 'r') as f:\n",
    "        for line in f:\n",
    "            word_idx_map[line[:len(line)- 1]] = index\n",
    "            index += 1\n",
    "    return word_idx_map\n",
    "fileName = 'w/train10.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_idx_map = getWordIdxMap(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getW(fileName):\n",
    "    w = list()\n",
    "    with open(fileName, 'r') as f:\n",
    "        for line in f:\n",
    "            w.append(line[ : len(line) - 1])\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = getW(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.000324217 0.000478765 -0.00101548 3.11326e-05 0.000833515 -0.00130511 -0.000374459 -1.84671e-05 -0.0014566 -0.00126843 0.0010159 -0.00135236 -0.00126625 0.000946137 0.000830792 0.00125306 -6.31069e-05 -0.00102398 0.000640504 -0.000159912 -0.000855165 -0.000245861 0.000770418 0.000877579 -0.00135257 0.000993456 -0.000165656 0.00134922 -0.0012755 0.00119272 -2.15209e-07 0.00100466 0.0011285 0.001633 -6.24224e-05 0.000192647 -0.000587514 0.000748321 -0.00113267 0.00156511 0.00153653 0.000865667 -0.000194187 -0.000590046 0.000820114 0.0011045 -0.000754885 0.00138497 -0.00119104 -0.00100889 0.00141111 -0.000380943 0.00101509 -0.000657161 -0.000183413 0.000802339 -0.000465722 0.0011138 0.0012733 0.00049756 0.000845964 0.000720214 -0.000412952 -0.00112085 0.000657455 0.000602698 4.0224e-05 -0.00110534 -0.000919386 0.00141692 -0.00118976 0.00117532 -0.000189027 0.00111379 -0.000216232 -0.000961618 0.00147645 -0.000992179 0.00120308 -0.000117368 0.00151307 -0.000789175 -0.00140118 -0.000979062 -0.00104863 0.0010324 -0.0016493 -0.000161111 8.96515e-05 0.000965399 -0.000505559 -0.00111225 -0.000225907 -0.000852344 -0.00163663 -0.000516735 0.000287698 -0.00109383 -0.00147137 0.000477101 -0.000221832 -0.00128632 -0.00162976 -0.00011415 -8.97291e-05 -0.00157814 0.00162899 0.000858235 -0.00117448 -1.14688e-05 0.000102047 -0.00152178 -0.00044943 -0.000610395 0.00126691 0.0013585 5.12187e-05 -0.00120432 0.00165811 -0.000742182 0.00157126 -0.00026562 0.000513023 -0.000896254 -0.00158266 0.00114875 0.000724861 -0.00125278 0.000612839 -0.00161586 -0.00051275 0.000762525 -0.00164044 0.000144965 0.000193213 -0.000188064 0.00155468 -9.55074e-05 0.000153265 0.000614894 -0.000756507 -0.000933841 0.000856482 0.00113755 0.000478291 -0.000380436 -0.001182 0.00126992 -0.00115863 -0.000486021 -0.00086242 -0.000696997 -0.000269027 0.000891589 0.00131115 -0.000317072 0.00113686 -0.000811545 -0.000311188 -8.43656e-05 0.00135344 0.00116474 0.000146682 0.00162963 0.000879549 0.000854674 -0.000863764 -0.000701094 -0.0011807 -0.000244335 -8.03715e-05 -0.00067022 0.000788199 0.000468711 0.00122713 -0.00122268 -0.000203603 0.00124423 -0.00126342 0.000316688 0.000482235 -0.00106622 -0.000415724 0.000977178 0.00104993 -0.000868864 -0.00111629 0.00150038 0.00161125 0.000249472 0.000617594 -0.000386082 0.00137537 0.000134522 0.000986291 -0.00139037 0.00152461 0.000670411 -0.00101919 -0.00146937 -0.00111502 -0.00046077 -7.72494e-05 -0.00107313 -0.000467914 0.00146542 -0.00123334 0.00020164 -0.000470507 -0.00156963 0.000256878 -0.000837969 0.00020696 -0.000233162 0.000148126 -0.000787342 7.30988e-05 -0.00142965 -0.00136111 0.000114726 0.00107262 0.000322786 -0.000434663 0.00146508 0.00155045 0.00149841 -0.00101391 0.000586042 -0.00156896 0.0011 0.00153979 7.87617e-06 -0.000923748 0.000743158 -0.000690974 -0.00142754 0.000481355 0.00153169 0.00156986 0.000356884 0.00121361 0.0005906 -0.00157455 -0.000682972 -0.000509058 -0.00105773 0.000103042 0.000282222 0.00101996 0.00136078 0.000117263 0.001417 0.00165616 -0.000771546 0.000804604 0.000564808 -0.00112179 0.00149076 0.00137336 -0.00089435 0.0011255 -0.00137656 0.00060541 -0.000948397 -0.000922634 -0.00135956 0.00159363 0.000758768 -0.00108023 -0.00134801 0.000801815 -0.000588996 0.000617206 0.00143096 7.2595e-05 -0.00133319 0.000254418 -0.000991359 0.00125217 0.000400469 0.000851818 0.000923593 0.00083257 0.0011197 0.00138643 -0.00129708 0.000525154 0.00107535 0.000459762 -0.000675807 -0.00127783 -0.000556864 -0.00122697 -0.000915837 -0.00116796 0.000842223 0.00150978 0.000837556 0.000332934 -0.00111813 0 \n"
     ]
    }
   ],
   "source": [
    "print(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['DESC:manner', 'Russia', 'How', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'Russia', '?'], ['ENTY:cremat', 'Popeye_Doyle', 'What', 'films', 'featured', 'the', 'character', 'Popeye', 'Doyle', '?'], ['DESC:manner', '', 'How', 'can', 'I', 'find', 'a', 'list', 'of', 'celebrities', \"'\", 'real', 'names', '?'], ['ENTY:animal', '', 'What', 'fowl', 'grabs', 'the', 'spotlight', 'after', 'the', 'Chinese', 'Year', 'of', 'the', 'Monkey', '?'], ['ABBR:exp', '', 'What', 'is', 'the', 'full', 'form', 'of', '.com', '?'], ['HUM:ind', '', 'What', 'contemptible', 'scoundrel', 'stole', 'the', 'cork', 'from', 'my', 'lunch', '?'], ['HUM:gr', 'St._Louis_Browns', 'What', 'team', 'did', 'baseball', \"'s\", 'St.', 'Louis', 'Browns', 'become', '?'], ['HUM:title', '', 'What', 'is', 'the', 'oldest', 'profession', '?'], ['DESC:def', '', 'What', 'are', 'liver', 'enzymes', '?'], ['HUM:ind', '', 'Name', 'the', 'scar-faced', 'bounty', 'hunter', 'of', 'The', 'Old', 'West', '.'], ['NUM:date', 'Ozzy_Osbourne', 'When', 'was', 'Ozzy', 'Osbourne', 'born', '?'], ['DESC:reason', '', 'Why', 'do', 'heavier', 'objects', 'travel', 'downhill', 'faster', '?'], ['HUM:ind', '', 'Who', 'was', 'The', 'Pride', 'of', 'the', 'Yankees', '?'], ['HUM:ind', 'Gandhi', 'Who', 'killed', 'Gandhi', '?'], ['ENTY:event', '', 'What', 'is', 'considered', 'the', 'costliest', 'disaster', 'the', 'insurance', 'industry', 'has', 'ever', 'faced', '?'], ['LOC:state', 'U.S.', 'What', 'sprawling', 'U.S.', 'state', 'boasts', 'the', 'most', 'airports', '?'], ['DESC:desc', 'U.S.', 'What', 'did', 'the', 'only', 'repealed', 'amendment', 'to', 'the', 'U.S.', 'Constitution', 'deal', 'with', '?'], ['NUM:count', '', 'How', 'many', 'Jews', 'were', 'executed', 'in', 'concentration', 'camps', 'during', 'WWII', '?'], ['DESC:def', '', 'What', 'is', '``', 'Nine', 'Inch', 'Nails', \"''\", '?'], ['DESC:def', '', 'What', 'is', 'an', 'annotated', 'bibliography', '?'], ['NUM:date', '', 'What', 'is', 'the', 'date', 'of', 'Boxing', 'Day', '?'], ['ENTY:other', '', 'What', 'articles', 'of', 'clothing', 'are', 'tokens', 'in', 'Monopoly', '?'], ['HUM:ind', '', 'Name', '11', 'famous', 'martyrs', '.'], ['DESC:desc', '', 'What', \"'s\", 'the', 'Olympic', 'motto', '?'], ['DESC:desc', 'Scarlett', 'What', 'is', 'the', 'origin', 'of', 'the', 'name', '`', 'Scarlett', \"'\", '?'], ['ENTY:letter', '', 'What', \"'s\", 'the', 'second-most-used', 'vowel', 'in', 'English', '?'], ['HUM:ind', '', 'Who', 'was', 'the', 'inventor', 'of', 'silly', 'putty', '?'], ['LOC:other', 'United_States', 'What', 'is', 'the', 'highest', 'waterfall', 'in', 'the', 'United', 'States', '?'], ['ENTY:other', 'Myrtle_Beach', 'Name', 'a', 'golf', 'course', 'in', 'Myrtle', 'Beach', '.'], ['LOC:state', 'Chesapeake_Bay', 'Which', 'two', 'states', 'enclose', 'Chesapeake', 'Bay', '?'], ['ABBR:exp', '', 'What', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', '?'], ['ENTY:other', '', 'What', 'does', 'a', 'spermologer', 'collect', '?']]\n"
     ]
    }
   ],
   "source": [
    "fileName = \"train1.txt\"\n",
    "with open(fileName, 'r') as fileInput:\n",
    "    sentences = []\n",
    "    for sentence in fileInput:\n",
    "        words = sentence.split()\n",
    "        strings = st3.tag(words)\n",
    "        sentence = [words[0]] + [retrieve(strings)] + words[1:]\n",
    "        sentences.append(sentence)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['DESC:manner', 'Russia', 'How', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'Russia', '?'], ['ENTY:cremat', 'Popeye_Doyle', 'What', 'films', 'featured', 'the', 'character', 'Popeye', 'Doyle', '?'], ['DESC:manner', '', 'How', 'can', 'I', 'find', 'a', 'list', 'of', 'celebrities', \"'\", 'real', 'names', '?'], ['ENTY:animal', '', 'What', 'fowl', 'grabs', 'the', 'spotlight', 'after', 'the', 'Chinese', 'Year', 'of', 'the', 'Monkey', '?'], ['ABBR:exp', '', 'What', 'is', 'the', 'full', 'form', 'of', '.com', '?'], ['HUM:ind', '', 'What', 'contemptible', 'scoundrel', 'stole', 'the', 'cork', 'from', 'my', 'lunch', '?'], ['HUM:gr', 'St._Louis_Browns', 'What', 'team', 'did', 'baseball', \"'s\", 'St.', 'Louis', 'Browns', 'become', '?'], ['HUM:title', '', 'What', 'is', 'the', 'oldest', 'profession', '?'], ['DESC:def', '', 'What', 'are', 'liver', 'enzymes', '?'], ['HUM:ind', '', 'Name', 'the', 'scar-faced', 'bounty', 'hunter', 'of', 'The', 'Old', 'West', '.'], ['NUM:date', 'Ozzy_Osbourne', 'When', 'was', 'Ozzy', 'Osbourne', 'born', '?'], ['DESC:reason', '', 'Why', 'do', 'heavier', 'objects', 'travel', 'downhill', 'faster', '?'], ['HUM:ind', '', 'Who', 'was', 'The', 'Pride', 'of', 'the', 'Yankees', '?'], ['HUM:ind', 'Gandhi', 'Who', 'killed', 'Gandhi', '?'], ['ENTY:event', '', 'What', 'is', 'considered', 'the', 'costliest', 'disaster', 'the', 'insurance', 'industry', 'has', 'ever', 'faced', '?'], ['LOC:state', 'U.S.', 'What', 'sprawling', 'U.S.', 'state', 'boasts', 'the', 'most', 'airports', '?'], ['DESC:desc', 'U.S.', 'What', 'did', 'the', 'only', 'repealed', 'amendment', 'to', 'the', 'U.S.', 'Constitution', 'deal', 'with', '?'], ['NUM:count', '', 'How', 'many', 'Jews', 'were', 'executed', 'in', 'concentration', 'camps', 'during', 'WWII', '?'], ['DESC:def', '', 'What', 'is', '``', 'Nine', 'Inch', 'Nails', \"''\", '?'], ['DESC:def', '', 'What', 'is', 'an', 'annotated', 'bibliography', '?'], ['NUM:date', '', 'What', 'is', 'the', 'date', 'of', 'Boxing', 'Day', '?'], ['ENTY:other', '', 'What', 'articles', 'of', 'clothing', 'are', 'tokens', 'in', 'Monopoly', '?'], ['HUM:ind', '', 'Name', '11', 'famous', 'martyrs', '.'], ['DESC:desc', '', 'What', \"'s\", 'the', 'Olympic', 'motto', '?'], ['DESC:desc', 'Scarlett', 'What', 'is', 'the', 'origin', 'of', 'the', 'name', '`', 'Scarlett', \"'\", '?'], ['ENTY:letter', '', 'What', \"'s\", 'the', 'second-most-used', 'vowel', 'in', 'English', '?'], ['HUM:ind', '', 'Who', 'was', 'the', 'inventor', 'of', 'silly', 'putty', '?'], ['LOC:other', 'United_States', 'What', 'is', 'the', 'highest', 'waterfall', 'in', 'the', 'United', 'States', '?'], ['ENTY:other', 'Myrtle_Beach', 'Name', 'a', 'golf', 'course', 'in', 'Myrtle', 'Beach', '.'], ['LOC:state', 'Chesapeake_Bay', 'Which', 'two', 'states', 'enclose', 'Chesapeake', 'Bay', '?'], ['ABBR:exp', '', 'What', 'does', 'the', 'abbreviation', 'AIDS', 'stand', 'for', '?'], ['ENTY:other', '', 'What', 'does', 'a', 'spermologer', 'collect', '?']]\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=1, workers=4)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fileOutput = open('test.txt', 'w')\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        stringArray = []\n",
    "        for num in model[word]:\n",
    "            stringArray.append('{:s}'.format(str(num)))\n",
    "        fileOutput.write(\" \".join(stringArray))\n",
    "        fileOutput.write('\\n')\n",
    "    fileOutput.write(\"\\n\\n\")\n",
    "fileOutput.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " sentences = [['first', 'sentence'], ['second', 'sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-0.0468569'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(model['The'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileName = \"raw_data/train5500.txt\"\n",
    "with open(fileName, 'r') as fileInput:\n",
    "    catagoriesSet = set()\n",
    "    finesSet = set()\n",
    "    for sentence in fileInput:\n",
    "        finesSet.add(sentence.split(' ')[0])\n",
    "        catagoriesSet.add(sentence.split(' ')[0].split(':')[0])\n",
    "    catagories = list()\n",
    "    fines = list()\n",
    "    for fine in finesSet:\n",
    "        fines.append(fine)\n",
    "    fines.sort()\n",
    "    for cato in catagoriesSet:\n",
    "        catagories.append(cato)\n",
    "    catagories.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classFile = 'raw_data/class.txt'\n",
    "with open(classFile, 'w') as out:\n",
    "    for catagory in catagories:\n",
    "        out.write(catagory + '\\n')\n",
    "out.close()\n",
    "fineFile = 'raw_data/fine.txt'\n",
    "with open(fineFile, 'w') as out:\n",
    "    for fine in fines:\n",
    "        out.write(fine + '\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "def build_data_cv(fileName, cv = 10, classSelect = True):\n",
    "    \n",
    "    fineLabelArray = readClass('raw_data/fine.txt')\n",
    "    classLabelArray = readClass('raw_data/class.txt')\n",
    "    revs = []\n",
    "    vocab = defaultdict(float)\n",
    "    \n",
    "    with open(fileName, \"rb\") as f:\n",
    "        for sentence in f:\n",
    "            sentence = sentence.decode(\"utf-8\")\n",
    "            labelClass = sentence.strip().split()[0].split(':')[0]\n",
    "            labelFine = sentence.strip().split()[0]\n",
    "            words = sentence.strip().split()[1:]\n",
    "            wordsSet = set(words)\n",
    "            for word in wordsSet:\n",
    "                vocab[word] += 1\n",
    "            if classSelect:\n",
    "                label = classLabelArray.index(labelClass)\n",
    "            else:\n",
    "                label = fineLabelArray.index(labelFine)\n",
    "            datum = {\"y\": label,\n",
    "                     \"text\": \" \".join(words),\n",
    "                     \"num_words\": len(words),\n",
    "                     \"split\": np.random.randint(0, cv)}\n",
    "            revs.append(datum)\n",
    "    return revs, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readClass(fileName):\n",
    "    with open(fileName, 'r') as f:\n",
    "        labels = list()\n",
    "        for line in f:\n",
    "            labels.append(line.strip())\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string = 'raw_data/train10.txt'\n",
    "inFile = string\n",
    "outFile = string.split('/')[:-1] + string.split('/')[-1].split('.')[0] + 'Vector' + '.txt'\n",
    "\n",
    "with open(inFile, 'rb') as fileInput:\n",
    "    sentences = []\n",
    "    for sentence in fileInput:\n",
    "        originSentence = sentence.decode(\"utf-8\").split()[1:]\n",
    "        sentences.append(originSentence)\n",
    "fileInput.close()\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=1, workers=4)\n",
    "model.save('myModel')\n",
    "\n",
    "fileOutput = open(outFile, 'w')\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        stringArray = []\n",
    "        for num in model[word]:\n",
    "            stringArray.append('{:2s}'.format(str(num)))\n",
    "        fileOutput.write(\" \".join(stringArray))\n",
    "        fileOutput.write('\\n')\n",
    "    fileOutput.write(\"\\n\")\n",
    "fileOutput.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
