{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to dealing with text data is to tokenize it into its constituent words. This is done by a tokenizer that attempts to break up the prose into word tokens. These tokens are then used as the feature space for a machine learning algorithm that attempts to classify the text, or to cluster it. A widely used machine learning package for Python is **`sklearn`**. **`sklearn`** has a built in tokenizer that is obfuscated from the end user in how it tokenizes the words within the document. This is not ideal, as the user should dictate how the tokenization occurs. Thankfully the built in tokenizer can be overrided with a more sensible tokenizer. The question remains, which tokenizer should we use that is capable of dealing with text data created by users on social media.\n",
    "\n",
    "**`nltk`** is another package in Python that is used for natural language processing and is primarily used as a teaching tool since none of the code is optimized. **`nltk`** has several types of tokenizers and can connect to the Stanford Natural Language Group's tools to use thier tokenizer, which is quite popular. Below we will investigate the different types of tokenizers available in **`sklearn`** and **`nltk`**, and we will choose which one we will use for building the identity algorithm.\n",
    "\n",
    "# Note\n",
    "----\n",
    "You are going to need to download Java from Oracle in order for this to work properly. Download both the jdk and the jre and place both folders after untarring them in `/usr/local/java`. Then in your `~/.bashrc` file, update your path to be:\n",
    "\n",
    "* export JAVA_HOME=/usr/local/java/jdk1.8.0_60/bin/java\n",
    "* export PATH=$PATH:/usr/local/l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import (RegexpTokenizer,\n",
    "                           SpaceTokenizer,\n",
    "                           TreebankWordTokenizer,\n",
    "                           WhitespaceTokenizer,\n",
    "                           WordPunctTokenizer,\n",
    "                           stanford,\n",
    "                           word_tokenize)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the Stanford Natural Language Processing tools, we must first download the jar files from their website: http://nlp.stanford.edu/. Any of their tools can be downloaded to use as the tokenizer. We will download the **parser** package and store it in a folder called `stanford_models/parser` for the this experiment. When instantiating the StanfordTokenizer class within **`nltk`** you must supply it with the path to the jar file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_jar = 'stanford_models/parser/stanford-parser.jar'\n",
    "regex_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "space_tokenizer = SpaceTokenizer()\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "wordpunct_tokenizer = WordPunctTokenizer()\n",
    "stanford_tokenizer = stanford.StanfordTokenizer(path_to_jar=path_to_jar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the tokenizers on a labeled dataset that will ultimately be used as the training set for our identity algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "me_df = pd.DataFrame()\n",
    "with open('labeled_data/ME.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        raw = line\n",
    "        regex = regex_tokenizer.tokenize(raw)\n",
    "        space = space_tokenizer.tokenize(raw)\n",
    "        treebank = treebank_tokenizer.tokenize(raw)\n",
    "        whitespace = whitespace_tokenizer.tokenize(raw)\n",
    "        wordpunct = wordpunct_tokenizer.tokenize(raw)\n",
    "        stanford = stanford_tokenizer.tokenize(raw)\n",
    "        word = word_tokenize(raw)\n",
    "        cv = CountVectorizer()\n",
    "        cv.fit_transform([line])\n",
    "        sklearn = cv.get_feature_names()\n",
    "        me_df = me_df.append(\\\n",
    "            {\n",
    "                'raw': raw,\n",
    "                'regex': regex,\n",
    "                'space': space,\n",
    "                'treebank': treebank,\n",
    "                'whitespace': whitespace,\n",
    "                'wordpunct': wordpunct,\n",
    "                'stanford': stanford,\n",
    "                'word': word,\n",
    "                'sklearn': sklearn,\n",
    "                'label': 'me'\n",
    "            },\n",
    "            ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_tokenizers(df, row):\n",
    "    r\"\"\"Prints the given row from the given dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input dataframe.\n",
    "    row : integer\n",
    "        The row integer to look at. Index begins at 1.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A print out of the row in the dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    columns = [c for c in list(df.keys()) if c != 'label']\n",
    "    for r in range(row, row+1):\n",
    "        for column in columns:\n",
    "            print column, '\\n', df[column][r], '\\n', '-'*80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw \n",
      "Oh my goodness! I havent heard or thought of this song in ages. Thanks!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "regex \n",
      "['Oh', 'my', 'goodness', '!', 'I', 'havent', 'heard', 'or', 'thought', 'of', 'this', 'song', 'in', 'ages', '.', 'Thanks', '!'] \n",
      "--------------------------------------------------------------------------------\n",
      "sklearn \n",
      "[u'ages', u'goodness', u'havent', u'heard', u'in', u'my', u'of', u'oh', u'or', u'song', u'thanks', u'this', u'thought'] \n",
      "--------------------------------------------------------------------------------\n",
      "space \n",
      "[u'Oh', u'my', u'goodness!', u'I', u'havent', u'heard', u'or', u'thought', u'of', u'this', u'song', u'in', u'ages.', u'Thanks!\\n'] \n",
      "--------------------------------------------------------------------------------\n",
      "stanford \n",
      "[u'Oh', u'my', u'goodness', u'!', u'I', u'havent', u'heard', u'or', u'thought', u'of', u'this', u'song', u'in', u'ages', u'.', u'Thanks', u'!'] \n",
      "--------------------------------------------------------------------------------\n",
      "treebank \n",
      "['Oh', 'my', 'goodness', '!', 'I', 'havent', 'heard', 'or', 'thought', 'of', 'this', 'song', 'in', 'ages.', 'Thanks', '!'] \n",
      "--------------------------------------------------------------------------------\n",
      "whitespace \n",
      "['Oh', 'my', 'goodness!', 'I', 'havent', 'heard', 'or', 'thought', 'of', 'this', 'song', 'in', 'ages.', 'Thanks!'] \n",
      "--------------------------------------------------------------------------------\n",
      "word \n",
      "['Oh', 'my', 'goodness', '!', 'I', 'havent', 'heard', 'or', 'thought', 'of', 'this', 'song', 'in', 'ages', '.', 'Thanks', '!'] \n",
      "--------------------------------------------------------------------------------\n",
      "wordpunct \n",
      "['Oh', 'my', 'goodness', '!', 'I', 'havent', 'heard', 'or', 'thought', 'of', 'this', 'song', 'in', 'ages', '.', 'Thanks', '!'] \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "compare_tokenizers(me_df, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the different tokenizers shows that there is a **major** problem with the data, in that there are no conjunctions at all. This is going to cause issues for the classifier, as it has no idea how to handle language that has conjunctions in it. The only way to remedy this is to classify more posts that have conjunctions in them, or put them back into the labeled data. This is an issue that we will explore further when we go to actually build the classifier.\n",
    "\n",
    "Notice that all the tokenizers retain punctuation, except for the one from **`sklearn`**. Some tokenizers such as **`space`** and **`whitespace`** attach the punctuation to the word it is next to. This is due to the way the tokenizer is built in that it looks for whitespace between tokens and assumes that a token is sandwhiched between whitespace. This is great for languages that use whitespace between words, however, it will not work so well on languages such as Chinese.\n",
    "\n",
    "All the tokenizers, except for the one built into **`sklearn`** also retain word case, single letter words, and ordering. One of the reasons why order is not retained in the **`sklearn`** tokenizer is how the data is stored when it is returned from the CountVectorizer. It is transformed into a pandas Series object which does not retain order.\n",
    "\n",
    "The Stanford tokenizer is able to pick up the periods in the data as opposed to the **`treebank`** tokenizer. The Stanford tokenizer is also used a decent amount and runs quickly. We will investigate a few other rows to see if it is the winner of this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw \n",
      "When I preach the gospel at a Harvest Crusade, I feel I have a solemn responsibility to give the gospel accuratelyto not distort it, to not take away from it, and to not add to it.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "regex \n",
      "['When', 'I', 'preach', 'the', 'gospel', 'at', 'a', 'Harvest', 'Crusade', ',', 'I', 'feel', 'I', 'have', 'a', 'solemn', 'responsibility', 'to', 'give', 'the', 'gospel', 'accuratelyto', 'not', 'distort', 'it', ',', 'to', 'not', 'take', 'away', 'from', 'it', ',', 'and', 'to', 'not', 'add', 'to', 'it', '.'] \n",
      "--------------------------------------------------------------------------------\n",
      "sklearn \n",
      "[u'accuratelyto', u'add', u'and', u'at', u'away', u'crusade', u'distort', u'feel', u'from', u'give', u'gospel', u'harvest', u'have', u'it', u'not', u'preach', u'responsibility', u'solemn', u'take', u'the', u'to', u'when'] \n",
      "--------------------------------------------------------------------------------\n",
      "space \n",
      "[u'When', u'I', u'preach', u'the', u'gospel', u'at', u'a', u'Harvest', u'Crusade,', u'I', u'feel', u'I', u'have', u'a', u'solemn', u'responsibility', u'to', u'give', u'the', u'gospel', u'accuratelyto', u'not', u'distort', u'it,', u'to', u'not', u'take', u'away', u'from', u'it,', u'and', u'to', u'not', u'add', u'to', u'it.\\n'] \n",
      "--------------------------------------------------------------------------------\n",
      "stanford \n",
      "[u'When', u'I', u'preach', u'the', u'gospel', u'at', u'a', u'Harvest', u'Crusade', u',', u'I', u'feel', u'I', u'have', u'a', u'solemn', u'responsibility', u'to', u'give', u'the', u'gospel', u'accuratelyto', u'not', u'distort', u'it', u',', u'to', u'not', u'take', u'away', u'from', u'it', u',', u'and', u'to', u'not', u'add', u'to', u'it', u'.'] \n",
      "--------------------------------------------------------------------------------\n",
      "treebank \n",
      "['When', 'I', 'preach', 'the', 'gospel', 'at', 'a', 'Harvest', 'Crusade', ',', 'I', 'feel', 'I', 'have', 'a', 'solemn', 'responsibility', 'to', 'give', 'the', 'gospel', 'accuratelyto', 'not', 'distort', 'it', ',', 'to', 'not', 'take', 'away', 'from', 'it', ',', 'and', 'to', 'not', 'add', 'to', 'it', '.'] \n",
      "--------------------------------------------------------------------------------\n",
      "whitespace \n",
      "['When', 'I', 'preach', 'the', 'gospel', 'at', 'a', 'Harvest', 'Crusade,', 'I', 'feel', 'I', 'have', 'a', 'solemn', 'responsibility', 'to', 'give', 'the', 'gospel', 'accuratelyto', 'not', 'distort', 'it,', 'to', 'not', 'take', 'away', 'from', 'it,', 'and', 'to', 'not', 'add', 'to', 'it.'] \n",
      "--------------------------------------------------------------------------------\n",
      "word \n",
      "['When', 'I', 'preach', 'the', 'gospel', 'at', 'a', 'Harvest', 'Crusade', ',', 'I', 'feel', 'I', 'have', 'a', 'solemn', 'responsibility', 'to', 'give', 'the', 'gospel', 'accuratelyto', 'not', 'distort', 'it', ',', 'to', 'not', 'take', 'away', 'from', 'it', ',', 'and', 'to', 'not', 'add', 'to', 'it', '.'] \n",
      "--------------------------------------------------------------------------------\n",
      "wordpunct \n",
      "['When', 'I', 'preach', 'the', 'gospel', 'at', 'a', 'Harvest', 'Crusade', ',', 'I', 'feel', 'I', 'have', 'a', 'solemn', 'responsibility', 'to', 'give', 'the', 'gospel', 'accuratelyto', 'not', 'distort', 'it', ',', 'to', 'not', 'take', 'away', 'from', 'it', ',', 'and', 'to', 'not', 'add', 'to', 'it', '.'] \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "compare_tokenizers(me_df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw \n",
      "Well, you know that usually I do 1 or 2 posts per day\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "regex \n",
      "['Well', ',', 'you', 'know', 'that', 'usually', 'I', 'do', '1', 'or', '2', 'posts', 'per', 'day'] \n",
      "--------------------------------------------------------------------------------\n",
      "sklearn \n",
      "[u'day', u'do', u'know', u'or', u'per', u'posts', u'that', u'usually', u'well', u'you'] \n",
      "--------------------------------------------------------------------------------\n",
      "space \n",
      "[u'Well,', u'you', u'know', u'that', u'usually', u'I', u'do', u'1', u'or', u'2', u'posts', u'per', u'day\\n'] \n",
      "--------------------------------------------------------------------------------\n",
      "stanford \n",
      "[u'Well', u',', u'you', u'know', u'that', u'usually', u'I', u'do', u'1', u'or', u'2', u'posts', u'per', u'day'] \n",
      "--------------------------------------------------------------------------------\n",
      "treebank \n",
      "['Well', ',', 'you', 'know', 'that', 'usually', 'I', 'do', '1', 'or', '2', 'posts', 'per', 'day'] \n",
      "--------------------------------------------------------------------------------\n",
      "whitespace \n",
      "['Well,', 'you', 'know', 'that', 'usually', 'I', 'do', '1', 'or', '2', 'posts', 'per', 'day'] \n",
      "--------------------------------------------------------------------------------\n",
      "word \n",
      "['Well', ',', 'you', 'know', 'that', 'usually', 'I', 'do', '1', 'or', '2', 'posts', 'per', 'day'] \n",
      "--------------------------------------------------------------------------------\n",
      "wordpunct \n",
      "['Well', ',', 'you', 'know', 'that', 'usually', 'I', 'do', '1', 'or', '2', 'posts', 'per', 'day'] \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "compare_tokenizers(me_df, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the sample above, it is clear that we should be using the Stanford Tokenizer to tokenize the text data. Below are a few examples of some outlier text that is not in the training data set, but highlight the fact that the Stanford tokenizer is the clear winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_tokenizers(s):\n",
    "    print 'regex\\t\\t', regex_tokenizer.tokenize(s)\n",
    "    print 'space\\t\\t', space_tokenizer.tokenize(s)\n",
    "    print 'treebank\\t', treebank_tokenizer.tokenize(s)\n",
    "    print 'whitespace\\t', whitespace_tokenizer.tokenize(s)\n",
    "    print 'wordpunct\\t', wordpunct_tokenizer.tokenize(s)\n",
    "    print 'stanford\\t', stanford_tokenizer.tokenize(s)\n",
    "    print 'word\\t\\t', word_tokenize(s)\n",
    "    cv = CountVectorizer()\n",
    "    cv.fit_transform([s])\n",
    "    print 'sklearn\\t\\t', cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex\t\t['I', \"'ve\", 'been', 'working', 'a', 'lot', 'lately', '.']\n",
      "space\t\t[u\"I've\", u'been', u'working', u'a', u'lot', u'lately.']\n",
      "treebank\t['I', \"'ve\", 'been', 'working', 'a', 'lot', 'lately', '.']\n",
      "whitespace\t[\"I've\", 'been', 'working', 'a', 'lot', 'lately.']\n",
      "wordpunct\t['I', \"'\", 've', 'been', 'working', 'a', 'lot', 'lately', '.']\n",
      "stanford\t[u'I', u\"'ve\", u'been', u'working', u'a', u'lot', u'lately', u'.']\n",
      "word\t\t['I', \"'ve\", 'been', 'working', 'a', 'lot', 'lately', '.']\n",
      "sklearn\t\t[u'been', u'lately', u'lot', u've', u'working']\n"
     ]
    }
   ],
   "source": [
    "test_tokenizers(\"I've been working a lot lately.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **`sklearn`** is able to detect contractions, but it unfortunately removes single letters from the tokens. This can be adjusted by changing how the CountVectorizer attempts to find tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex\t\t['What', 'about', 'super', '-long-hyphenated', 'words', '?']\n",
      "space\t\t[u'What', u'about', u'super-long-hyphenated', u'words?']\n",
      "treebank\t['What', 'about', 'super-long-hyphenated', 'words', '?']\n",
      "whitespace\t['What', 'about', 'super-long-hyphenated', 'words?']\n",
      "wordpunct\t['What', 'about', 'super', '-', 'long', '-', 'hyphenated', 'words', '?']\n",
      "stanford\t[u'What', u'about', u'super-long-hyphenated', u'words', u'?']\n",
      "word\t\t['What', 'about', 'super-long-hyphenated', 'words', '?']\n",
      "sklearn\t\t[u'about', u'hyphenated', u'long', u'super', u'what', u'words']\n"
     ]
    }
   ],
   "source": [
    "test_tokenizers(\"What about super-long-hyphenated words?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the Stanford Tokenizer is still winning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex\t\t[u'Em', u'dashes', u'\\u2014rock', u'and', u'roll', u'\\u2014are', u'parenthetical', u'.']\n",
      "space\t\t[u'Em', u'dashes\\u2014rock', u'and', u'roll\\u2014are', u'parenthetical.']\n",
      "treebank\t[u'Em', u'dashes\\u2014rock', u'and', u'roll\\u2014are', u'parenthetical', u'.']\n",
      "whitespace\t[u'Em', u'dashes\\u2014rock', u'and', u'roll\\u2014are', u'parenthetical.']\n",
      "wordpunct\t[u'Em', u'dashes', u'\\u2014', u'rock', u'and', u'roll', u'\\u2014', u'are', u'parenthetical', u'.']\n",
      "stanford\t[u'Em', u'dashes', u'--', u'rock', u'and', u'roll', u'--', u'are', u'parenthetical', u'.']\n",
      "word\t\t[u'Em', u'dashes\\u2014rock', u'and', u'roll\\u2014are', u'parenthetical', u'.']\n",
      "sklearn\t\t[u'and', u'are', u'dashes', u'em', u'parenthetical', u'rock', u'roll']\n"
     ]
    }
   ],
   "source": [
    "test_tokenizers(u\"Em dashes—rock and roll—are parenthetical.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, the Stanford Tokenizer was able to detect an em-dash situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex\t\t[u'How', u'about', u'actual', u'(parenthetical', u'statement', u')', u'parenthesis', u'?']\n",
      "space\t\t[u'How', u'about', u'actual', u'(parenthetical', u'statement)', u'parenthesis?']\n",
      "treebank\t[u'How', u'about', u'actual', u'(', u'parenthetical', u'statement', u')', u'parenthesis', u'?']\n",
      "whitespace\t[u'How', u'about', u'actual', u'(parenthetical', u'statement)', u'parenthesis?']\n",
      "wordpunct\t[u'How', u'about', u'actual', u'(', u'parenthetical', u'statement', u')', u'parenthesis', u'?']\n",
      "stanford\t[u'How', u'about', u'actual', u'-LRB-', u'parenthetical', u'statement', u'-RRB-', u'parenthesis', u'?']\n",
      "word\t\t[u'How', u'about', u'actual', u'(', u'parenthetical', u'statement', u')', u'parenthesis', u'?']\n",
      "sklearn\t\t[u'about', u'actual', u'how', u'parenthesis', u'parenthetical', u'statement']\n"
     ]
    }
   ],
   "source": [
    "test_tokenizers(u\"How about actual (parenthetical statement) parenthesis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex\t\t[u'{foo}', u':)', u'[bar]', u'\\u2014_\\u2014']\n",
      "space\t\t[u'{foo}', u':)', u'[bar]', u'\\u2014_\\u2014']\n",
      "treebank\t[u'{', u'foo', u'}', u':', u')', u'[', u'bar', u']', u'\\u2014_\\u2014']\n",
      "whitespace\t[u'{foo}', u':)', u'[bar]', u'\\u2014_\\u2014']\n",
      "wordpunct\t[u'{', u'foo', u'}', u':)', u'[', u'bar', u']', u'\\u2014', u'_', u'\\u2014']\n",
      "stanford\t[u'-LCB-', u'foo', u'-RCB-', u':-RRB-', u'-LSB-', u'bar', u'-RSB-', u'--', u'_', u'--']\n",
      "word\t\t[u'{', u'foo', u'}', u':', u')', u'[', u'bar', u']', u'\\u2014_\\u2014']\n",
      "sklearn\t\t[u'bar', u'foo']\n"
     ]
    }
   ],
   "source": [
    "test_tokenizers(u\"{foo} :) [bar] —_—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to deal with emoticons has not been established in the tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex\t\t[u'En', u'-dash', u'2', u'\\u20133!']\n",
      "space\t\t[u'En-dash', u'2\\u20133!']\n",
      "treebank\t[u'En-dash', u'2\\u20133', u'!']\n",
      "whitespace\t[u'En-dash', u'2\\u20133!']\n",
      "wordpunct\t[u'En', u'-', u'dash', u'2', u'\\u2013', u'3', u'!']\n",
      "stanford\t[u'En-dash', u'2', u'--', u'3', u'!']\n",
      "word\t\t[u'En-dash', u'2\\u20133', u'!']\n",
      "sklearn\t\t[u'dash', u'en']\n"
     ]
    }
   ],
   "source": [
    "test_tokenizers(u\"En-dash 2–3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex\t\t[u'That', u\"'s\", u'gr8', u'!', u'LOL', u'!!!!!']\n",
      "space\t\t[u\"That's\", u'gr8!', u'LOL!!!!!']\n",
      "treebank\t[u'That', u\"'s\", u'gr8', u'!', u'LOL', u'!', u'!', u'!', u'!', u'!']\n",
      "whitespace\t[u\"That's\", u'gr8!', u'LOL!!!!!']\n",
      "wordpunct\t[u'That', u\"'\", u's', u'gr8', u'!', u'LOL', u'!!!!!']\n",
      "stanford\t[u'That', u\"'s\", u'gr8', u'!', u'LOL', u'!!!!!']\n",
      "word\t\t[u'That', u\"'s\", u'gr8', u'!', u'LOL', u'!', u'!', u'!', u'!', u'!']\n",
      "sklearn\t\t[u'gr8', u'lol', u'that']\n"
     ]
    }
   ],
   "source": [
    "test_tokenizers(u\"That's gr8! LOL!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex\t\t[u'reeeeeeeeaaaaaaaaaalllllllllyyyyyyyyyyyy', u'???????']\n",
      "space\t\t[u'reeeeeeeeaaaaaaaaaalllllllllyyyyyyyyyyyy???????']\n",
      "treebank\t[u'reeeeeeeeaaaaaaaaaalllllllllyyyyyyyyyyyy', u'?', u'?', u'?', u'?', u'?', u'?', u'?']\n",
      "whitespace\t[u'reeeeeeeeaaaaaaaaaalllllllllyyyyyyyyyyyy???????']\n",
      "wordpunct\t[u'reeeeeeeeaaaaaaaaaalllllllllyyyyyyyyyyyy', u'???????']\n",
      "stanford\t[u'reeeeeeeeaaaaaaaaaalllllllllyyyyyyyyyyyy', u'???????']\n",
      "word\t\t[u'reeeeeeeeaaaaaaaaaalllllllllyyyyyyyyyyyy', u'?', u'?', u'?', u'?', u'?', u'?', u'?']\n",
      "sklearn\t\t[u'reeeeeeeeaaaaaaaaaalllllllllyyyyyyyyyyyy']\n"
     ]
    }
   ],
   "source": [
    "test_tokenizers(u\"reeeeeeeeaaaaaaaaaalllllllllyyyyyyyyyyyy???????\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
